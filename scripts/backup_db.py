#!/usr/bin/env python3
# scripts/backup_db.py
"""
ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ìŠ¤í¬ë¦½íŠ¸ (í”„ë¡œë•ì…˜ ê¸‰)

ê¸°ëŠ¥:
- PostgreSQL: pg_dump ì‚¬ìš© (custom format)
- SQLite: íŒŒì¼ ë³µì‚¬ + ì••ì¶•
- ë°±ì—… íŒŒì¼ ìë™ ì‚­ì œ (ë³´ê´€ ê¸°ê°„ ê²½ê³¼)
- ë°±ì—… ê²€ì¦ (íŒŒì¼ í¬ê¸°, ë¬´ê²°ì„±)
- Slack ì•Œë¦¼ (ì„±ê³µ/ì‹¤íŒ¨)
- íƒ€ì„ì¡´ ëª…ì‹œ (UTC)

Usage:
    python scripts/backup_db.py
    python scripts/backup_db.py --retention-days 14
"""
import sys
import subprocess
import shutil
import gzip
from pathlib import Path
from datetime import datetime, timedelta, timezone
from urllib.parse import urlparse
from typing import Optional, Dict, Any
import argparse

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from app.core.config import settings
from app.utils.logger import get_logger

logger = get_logger(__name__)

# ë°±ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
BACKUP_DIR = Path(settings.LOG_DIR).parent / "backups"
if not BACKUP_DIR.is_absolute():
    BACKUP_DIR = PROJECT_ROOT / BACKUP_DIR

# ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
try:
    BACKUP_DIR.mkdir(parents=True, exist_ok=True)
    logger.info(f"ë°±ì—… ë””ë ‰í† ë¦¬: {BACKUP_DIR}")
except PermissionError as e:
    logger.error(f"ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„± ì‹¤íŒ¨: {BACKUP_DIR} - {e}")
    sys.exit(1)

# Slack ì•Œë¦¼ ì„¤ì •
SLACK_NOTIFY = settings.SLACK_WEBHOOK_URL is not None

# Slack ìœ í‹¸ import
try:
    from app.utils.slack_utils import send_slack_message
except ImportError:
    logger.warning("Slack ìœ í‹¸ë¦¬í‹°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì•Œë¦¼ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
    send_slack_message = None
    SLACK_NOTIFY = False


def backup_postgres(parsed: urlparse, backup_path: Path) -> Dict[str, Any]:
    """
    PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… (pg_dump)

    Args:
        parsed: urlparse ê²°ê³¼ (DATABASE_URL)
        backup_path: ë°±ì—… íŒŒì¼ ê²½ë¡œ

    Returns:
        {
            "success": True/False,
            "file_path": backup_path,
            "file_size_mb": íŒŒì¼ í¬ê¸° (MB),
            "duration_sec": ì†Œìš” ì‹œê°„ (ì´ˆ)
        }

    Raises:
        subprocess.CalledProcessError: pg_dump ì‹¤í–‰ ì‹¤íŒ¨
    """
    start_time = datetime.now(timezone.utc)

    user = parsed.username
    password = parsed.password
    host = parsed.hostname or "localhost"
    port = parsed.port or 5432
    dbname = parsed.path.lstrip("/")

    # PGPASSWORD í™˜ê²½ë³€ìˆ˜ ì„¤ì •
    env = dict(subprocess.os.environ)
    if password:
        env["PGPASSWORD"] = password

    # pg_dump ëª…ë ¹ì–´
    cmd = [
        "pg_dump",
        "-h", host,
        "-p", str(port),
        "-U", user,
        "-F", "c",  # custom format (ì••ì¶•ë¨)
        "-b",  # include blobs
        "-v",  # verbose
        "-f", str(backup_path),
        dbname,
    ]

    logger.info(f"PostgreSQL ë°±ì—… ì‹œì‘ - DB: {dbname}, Host: {host}")
    logger.debug(f"pg_dump ëª…ë ¹: {' '.join(cmd)}")

    try:
        result = subprocess.run(
            cmd,
            env=env,
            capture_output=True,
            text=True,
            check=True,
        )

        # ë°±ì—… íŒŒì¼ ê²€ì¦
        if not backup_path.exists():
            raise FileNotFoundError(f"ë°±ì—… íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {backup_path}")

        file_size_bytes = backup_path.stat().st_size
        file_size_mb = file_size_bytes / (1024 * 1024)

        if file_size_bytes == 0:
            raise ValueError("ë°±ì—… íŒŒì¼ í¬ê¸°ê°€ 0ë°”ì´íŠ¸ì…ë‹ˆë‹¤")

        duration = (datetime.now(timezone.utc) - start_time).total_seconds()

        logger.info(
            f"PostgreSQL ë°±ì—… ì™„ë£Œ - íŒŒì¼: {backup_path.name}, "
            f"í¬ê¸°: {file_size_mb:.2f}MB, ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ"
        )

        return {
            "success": True,
            "file_path": str(backup_path),
            "file_size_mb": round(file_size_mb, 2),
            "duration_sec": round(duration, 2),
        }

    except subprocess.CalledProcessError as e:
        logger.error(f"pg_dump ì‹¤í–‰ ì‹¤íŒ¨: {e.stderr}")
        raise


def backup_sqlite(db_path: Path, backup_path: Path) -> Dict[str, Any]:
    """
    SQLite ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… (íŒŒì¼ ë³µì‚¬ + gzip ì••ì¶•)

    Args:
        db_path: SQLite DB íŒŒì¼ ê²½ë¡œ
        backup_path: ë°±ì—… íŒŒì¼ ê²½ë¡œ

    Returns:
        {
            "success": True/False,
            "file_path": backup_path,
            "file_size_mb": íŒŒì¼ í¬ê¸° (MB),
            "duration_sec": ì†Œìš” ì‹œê°„ (ì´ˆ)
        }
    """
    start_time = datetime.now(timezone.utc)

    if not db_path.exists():
        raise FileNotFoundError(f"SQLite DB íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {db_path}")

    logger.info(f"SQLite ë°±ì—… ì‹œì‘ - DB: {db_path}")

    # ë°±ì—… íŒŒì¼ì„ gzipìœ¼ë¡œ ì••ì¶•
    with open(db_path, "rb") as f_in:
        with gzip.open(str(backup_path) + ".gz", "wb") as f_out:
            shutil.copyfileobj(f_in, f_out)

    backup_path_gz = Path(str(backup_path) + ".gz")

    # ë°±ì—… íŒŒì¼ ê²€ì¦
    if not backup_path_gz.exists():
        raise FileNotFoundError(f"ë°±ì—… íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {backup_path_gz}")

    file_size_bytes = backup_path_gz.stat().st_size
    file_size_mb = file_size_bytes / (1024 * 1024)

    duration = (datetime.now(timezone.utc) - start_time).total_seconds()

    logger.info(
        f"SQLite ë°±ì—… ì™„ë£Œ - íŒŒì¼: {backup_path_gz.name}, "
        f"í¬ê¸°: {file_size_mb:.2f}MB, ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ"
    )

    return {
        "success": True,
        "file_path": str(backup_path_gz),
        "file_size_mb": round(file_size_mb, 2),
        "duration_sec": round(duration, 2),
    }


def cleanup_old_backups(retention_days: int = None):
    """
    ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ìë™ ì‚­ì œ

    Args:
        retention_days: ë°±ì—… ë³´ê´€ ê¸°ê°„ (ì¼). Noneì´ë©´ settings.BACKUP_RETENTION_DAYS ì‚¬ìš©
    """
    if retention_days is None:
        retention_days = settings.BACKUP_RETENTION_DAYS

    cutoff_date = datetime.now(timezone.utc) - timedelta(days=retention_days)

    logger.info(f"ë°±ì—… íŒŒì¼ ì •ë¦¬ ì‹œì‘ - ë³´ê´€ ê¸°ê°„: {retention_days}ì¼")

    deleted_count = 0
    deleted_size = 0

    for backup_file in BACKUP_DIR.glob("*_backup_*"):
        try:
            file_mtime = datetime.fromtimestamp(
                backup_file.stat().st_mtime, tz=timezone.utc
            )

            if file_mtime < cutoff_date:
                file_size = backup_file.stat().st_size
                backup_file.unlink()
                deleted_count += 1
                deleted_size += file_size
                logger.debug(f"ì‚­ì œëœ ë°±ì—… íŒŒì¼: {backup_file.name}")

        except Exception as e:
            logger.warning(f"ë°±ì—… íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {backup_file.name} - {e}")

    deleted_size_mb = deleted_size / (1024 * 1024)
    logger.info(
        f"ë°±ì—… íŒŒì¼ ì •ë¦¬ ì™„ë£Œ - ì‚­ì œ: {deleted_count}ê°œ, "
        f"í™•ë³´ ê³µê°„: {deleted_size_mb:.2f}MB"
    )

    return {"deleted_count": deleted_count, "deleted_size_mb": round(deleted_size_mb, 2)}


def run_backup(retention_days: Optional[int] = None) -> Dict[str, Any]:
    """
    ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì‹¤í–‰ (ë©”ì¸ í•¨ìˆ˜)

    Args:
        retention_days: ë°±ì—… ë³´ê´€ ê¸°ê°„ (ì¼)

    Returns:
        {
            "success": True/False,
            "db_type": "postgresql" | "sqlite",
            "backup_result": backup_postgres() ë˜ëŠ” backup_sqlite() ê²°ê³¼,
            "cleanup_result": cleanup_old_backups() ê²°ê³¼,
            "error": ì—ëŸ¬ ë©”ì‹œì§€ (ì‹¤íŒ¨ ì‹œ)
        }
    """
    timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
    parsed = urlparse(settings.DATABASE_URL)

    result = {
        "success": False,
        "db_type": None,
        "backup_result": None,
        "cleanup_result": None,
        "timestamp": timestamp,
    }

    try:
        # 1. ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…
        if parsed.scheme in ("postgres", "postgresql"):
            result["db_type"] = "postgresql"
            backup_path = BACKUP_DIR / f"pg_backup_{timestamp}.dump"
            result["backup_result"] = backup_postgres(parsed, backup_path)

        elif parsed.scheme == "sqlite":
            result["db_type"] = "sqlite"

            # SQLite ê²½ë¡œ íŒŒì‹±
            sqlite_path = parsed.path.lstrip("/")
            if not Path(sqlite_path).is_absolute():
                sqlite_path = PROJECT_ROOT / sqlite_path

            backup_path = BACKUP_DIR / f"sqlite_backup_{timestamp}.db"
            result["backup_result"] = backup_sqlite(Path(sqlite_path), backup_path)

        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” DB ìŠ¤í‚¤ë§ˆ: {parsed.scheme}")

        # 2. ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì •ë¦¬
        result["cleanup_result"] = cleanup_old_backups(retention_days)

        result["success"] = True

        # 3. Slack ì•Œë¦¼ (ì„±ê³µ)
        if SLACK_NOTIFY and send_slack_message:
            backup_info = result["backup_result"]
            cleanup_info = result["cleanup_result"]

            message = f"""âœ… **DB ë°±ì—… ì„±ê³µ**

ğŸ“Š ë°±ì—… ì •ë³´:
- DB íƒ€ì…: {result['db_type'].upper()}
- íŒŒì¼: `{Path(backup_info['file_path']).name}`
- í¬ê¸°: {backup_info['file_size_mb']}MB
- ì†Œìš” ì‹œê°„: {backup_info['duration_sec']}ì´ˆ

ğŸ—‘ï¸ ì •ë¦¬:
- ì‚­ì œëœ ë°±ì—…: {cleanup_info['deleted_count']}ê°œ
- í™•ë³´ ê³µê°„: {cleanup_info['deleted_size_mb']}MB
- ë³´ê´€ ê¸°ê°„: {retention_days or settings.BACKUP_RETENTION_DAYS}ì¼

â° ë°±ì—… ì‹œê°: {timestamp} UTC
"""
            send_slack_message(message)

        return result

    except subprocess.CalledProcessError as e:
        error_msg = f"pg_dump ì‹¤í–‰ ì‹¤íŒ¨: {e.stderr}"
        result["error"] = error_msg
        logger.error(error_msg, exc_info=True)

        if SLACK_NOTIFY and send_slack_message:
            send_slack_message(f"âŒ **DB ë°±ì—… ì‹¤íŒ¨**\n\n{error_msg}")

        raise

    except Exception as e:
        error_msg = f"ë°±ì—… ì‹¤íŒ¨: {str(e)}"
        result["error"] = error_msg
        logger.error(error_msg, exc_info=True)

        if SLACK_NOTIFY and send_slack_message:
            send_slack_message(f"âŒ **DB ë°±ì—… ì‹¤íŒ¨**\n\n{error_msg}")

        raise


def main():
    """CLI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸"""
    parser = argparse.ArgumentParser(description="ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument(
        "--retention-days",
        type=int,
        default=None,
        help=f"ë°±ì—… ë³´ê´€ ê¸°ê°„ (ì¼). ê¸°ë³¸ê°’: {settings.BACKUP_RETENTION_DAYS}",
    )
    args = parser.parse_args()

    logger.info("=" * 80)
    logger.info("ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì‹œì‘")
    logger.info("=" * 80)

    try:
        result = run_backup(retention_days=args.retention_days)

        logger.info("=" * 80)
        logger.info("ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì™„ë£Œ")
        logger.info("=" * 80)

        return 0

    except Exception as e:
        logger.critical("ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì‹¤íŒ¨", exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(main())
