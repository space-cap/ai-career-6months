"""
evaluate_response.py
--------------------------------
ì‚¬ìš©ì ì‘ë‹µ í’ˆì§ˆ í‰ê°€ ëª¨ë“ˆ
AI ì‘ë‹µì˜ ìì—°ìŠ¤ëŸ¬ì›€ / ë„ì›€ ì •ë„ / ê°ì • ì¼ì¹˜ë„ ë“± ì ìˆ˜í™”

ì‹¤í–‰ ë°©ë²•:
  python -m app.evaluate_response
--------------------------------
"""
import os
import json
import pandas as pd
from sqlalchemy import create_engine
from openai import OpenAI
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# -----------------------------------
# í™˜ê²½ ë³€ìˆ˜ ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# -----------------------------------
DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///./local.db")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    raise ValueError("âŒ OPENAI_API_KEY is not set in .env file")

client = OpenAI(api_key=OPENAI_API_KEY)
engine = create_engine(DATABASE_URL)


def evaluate_response(answer: str, question: str) -> dict:
    """
    OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ AI ì‘ë‹µì˜ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.

    Args:
        answer (str): AIê°€ ìƒì„±í•œ ë‹µë³€
        question (str): ì‚¬ìš©ìì˜ ì§ˆë¬¸

    Returns:
        dict: í‰ê°€ ê²°ê³¼
            - relevance (int): ì§ˆë¬¸ ê´€ë ¨ì„± (0~10)
            - clarity (int): ëª…í™•ì„± ë° ë„ì›€ ì •ë„ (0~10)
            - emotion (int): ê°ì • í†¤ ì¼ì¹˜ë„ (0~10)
            - comment (str): í‰ê°€ ì½”ë©˜íŠ¸
    """
    # OpenAI API í˜¸ì¶œìš© í”„ë¡¬í”„íŠ¸ ì‘ì„±
    prompt = f"""
    Evaluate the AI's answer quality based on these 3 aspects:
    1. Relevance to the user's question (0~10)
    2. Helpfulness and clarity (0~10)
    3. Emotional tone alignment (0~10)

    Question: {question}
    Answer: {answer}

    Return the evaluation in JSON format:
    {{
        "relevance": <score 0-10>,
        "clarity": <score 0-10>,
        "emotion": <score 0-10>,
        "comment": "<brief evaluation comment>"
    }}
    """

    try:
        # âœ… ìµœì‹  OpenAI API ë¬¸ë²• (chat.completions.create)
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are an AI response quality evaluator. Return only valid JSON."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3  # ì¼ê´€ëœ í‰ê°€ë¥¼ ìœ„í•´ ë‚®ì€ temperature
        )

        # ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        result_text = response.choices[0].message.content.strip()

        # JSON íŒŒì‹± ì‹œë„
        try:
            # JSON ì½”ë“œ ë¸”ë¡ ì œê±° (```json ... ``` í˜•ì‹)
            if result_text.startswith("```json"):
                result_text = result_text.replace("```json", "").replace("```", "").strip()
            elif result_text.startswith("```"):
                result_text = result_text.replace("```", "").strip()

            scores = json.loads(result_text)

            # ê¸°ë³¸ê°’ ì„¤ì • (í‚¤ê°€ ì—†ì„ ê²½ìš° ëŒ€ë¹„)
            return {
                "relevance": scores.get("relevance", 0),
                "clarity": scores.get("clarity", 0),
                "emotion": scores.get("emotion", 0),
                "comment": scores.get("comment", "No comment")[:200]  # ìµœëŒ€ 200ì
            }

        except json.JSONDecodeError:
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            print(f"âš ï¸ JSON parsing failed. Raw response: {result_text[:100]}")
            return {
                "relevance": 0,
                "clarity": 0,
                "emotion": 0,
                "comment": result_text[:200]
            }

    except Exception as e:
        print(f"âŒ Evaluation error: {e}")
        return {
            "relevance": 0,
            "clarity": 0,
            "emotion": 0,
            "comment": f"Error: {str(e)[:100]}"
        }


def run_evaluation():
    """
    ìµœê·¼ ëŒ€í™” ë¡œê·¸ë¥¼ ì¡°íšŒí•˜ì—¬ AI ì‘ë‹µ í’ˆì§ˆì„ í‰ê°€í•˜ê³  ê²°ê³¼ë¥¼ DBì— ì €ì¥í•©ë‹ˆë‹¤.

    ë™ì‘:
        1. conversation_log í…Œì´ë¸”ì—ì„œ ìµœê·¼ 10ê°œì˜ ëŒ€í™” ë¡œê·¸ ì¡°íšŒ
        2. ê° ëŒ€í™”ì— ëŒ€í•´ evaluate_response() í•¨ìˆ˜ í˜¸ì¶œ
        3. í‰ê°€ ê²°ê³¼ë¥¼ conversation_evaluation í…Œì´ë¸”ì— ì €ì¥
    """
    print("ğŸ“Š Starting evaluation process...")

    # 1. ìµœê·¼ 10ê°œì˜ ëŒ€í™” ë¡œê·¸ ì¡°íšŒ (í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ)
    df = pd.read_sql(
        """
        SELECT id, user_id, question, answer, sentiment, topic, created_at
        FROM conversation_log
        ORDER BY created_at DESC
        LIMIT 10
        """,
        engine
    )

    if df.empty:
        print("âš ï¸ No conversation data found in the database.")
        return

    print(f"âœ… Found {len(df)} conversations to evaluate.")

    # 2. ê° ëŒ€í™”ì— ëŒ€í•´ í‰ê°€ ì‹¤í–‰
    evaluations = []
    for idx, row in df.iterrows():
        print(f"   Evaluating conversation {idx + 1}/{len(df)}...")
        eval_result = evaluate_response(row["answer"], row["question"])
        evaluations.append(eval_result)

    # 3. í‰ê°€ ê²°ê³¼ë¥¼ ìƒˆ ë°ì´í„°í”„ë ˆì„ ìƒì„± (ì›ë³¸ ë°ì´í„° + í‰ê°€ ê²°ê³¼)
    evaluation_df = pd.DataFrame({
        "conversation_id": df["id"],  # ì›ë³¸ ëŒ€í™” ID ì°¸ì¡°
        "user_id": df["user_id"],
        "question": df["question"],
        "answer": df["answer"],
        "sentiment": df["sentiment"],
        "topic": df["topic"],
        "created_at": df["created_at"],
        # í‰ê°€ ê²°ê³¼ ì¶”ê°€
        "relevance": [e["relevance"] for e in evaluations],
        "clarity": [e["clarity"] for e in evaluations],
        "emotion": [e["emotion"] for e in evaluations],
        "comment": [e["comment"] for e in evaluations]
    })

    # 4. ê²°ê³¼ë¥¼ conversation_evaluation í…Œì´ë¸”ì— ì €ì¥ (ê¸°ì¡´ í…Œì´ë¸” ë®ì–´ì“°ê¸°)
    evaluation_df.to_sql("conversation_evaluation", engine, if_exists="replace", index=False)

    print("âœ… Evaluation results saved to 'conversation_evaluation' table.")
    print(f"   Average scores - Relevance: {evaluation_df['relevance'].mean():.1f}, "
          f"Clarity: {evaluation_df['clarity'].mean():.1f}, Emotion: {evaluation_df['emotion'].mean():.1f}")


if __name__ == "__main__":
    run_evaluation()
